{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = './data/'\n",
    "GLOVE_DIR = './glove_w2v/'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecrods: 400000\n"
     ]
    }
   ],
   "source": [
    "embds_index = {}\n",
    "with codecs.open(os.path.join(GLOVE_DIR, 'glove.6b.300d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word, coefs = values[0], np.array(values[1:], dtype='float32')\n",
    "        embds_index[word] = coefs\n",
    "print('Vecrods: {}'.format(len(embds_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_texts(filename, text1_ind, text2_ind, label_ind):\n",
    "    texts1, texts2, labels = [], [], []\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f,  delimiter=',') #as reader:\n",
    "        header = next(reader)\n",
    "        for values in reader:\n",
    "            texts1.append(values[text1_ind])\n",
    "            texts2.append(values[text2_ind])\n",
    "            labels.append(int(values[label_ind]))\n",
    "    return texts1, texts2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text\n",
      "Texts: 404290\n"
     ]
    }
   ],
   "source": [
    "print('Processing text')\n",
    "texts1, texts2, labels = load_texts(TRAIN_DATA_FILE,\n",
    "                                    text1_ind=2, \n",
    "                                    text2_ind=3, \n",
    "                                    label_ind=5)\n",
    "print('Texts: {}'.format(len(texts1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text\n",
      "Test Texts: 2345796\n"
     ]
    }
   ],
   "source": [
    "print('Processing text')\n",
    "test_texts1, test_texts2, test_labels = load_texts(TEST_DATA_FILE,\n",
    "                                                   text1_ind=1,\n",
    "                                                   text2_ind=2,\n",
    "                                                   label_ind=0)\n",
    "print('Test Texts: {}'.format(len(test_texts1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421364 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts1 + texts2 + test_texts1 + test_texts2)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts2)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences1 = tokenizer.texts_to_sequences(test_texts1)\n",
    "test_sequences2 = tokenizer.texts_to_sequences(test_texts2)\n",
    "\n",
    "data1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor: {}'.format(data1.shape))\n",
    "print('Shape of label tensor: {}'.format(labels.shape))\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_labels = np.array(test_labels)\n",
    "del test_sequences1\n",
    "del test_sequences2\n",
    "del sequences_1\n",
    "del sequences_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Null word embeddings: 133734\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embds_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: {}'.format(np.sum(\n",
    "            np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\dev\\anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:429: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `sum`, `concatenate`, etc.\n",
      "  warnings.warn('The `merge` function is deprecated '\n",
      "D:\\soft\\dev\\anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:66: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  warnings.warn('The `Merge` layer is deprecated '\n",
      "D:\\soft\\dev\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    }
   ],
   "source": [
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences1 = embedding_layer(sequence_1_input)\n",
    "x1 = Conv1D(128, 3, activation='relu')(embedded_sequences1)\n",
    "x1 = MaxPooling1D(10)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences2 = embedding_layer(sequence_2_input)\n",
    "y1 = Conv1D(128, 3, activation='relu')(embedded_sequences2)\n",
    "y1 = MaxPooling1D(10)(y1)\n",
    "y1 = Flatten()(y1)\n",
    "y1 = Dense(64, activation='relu')(y1)\n",
    "y1 = Dropout(0.2)(y1)\n",
    "\n",
    "merged = merge([x1, y1], mode='concat')\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(input=[sequence_1_input, sequence_2_input], output=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400247 samples, validate on 4043 samples\n",
      "Epoch 1/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.6059 - acc: 0.6514 - val_loss: 0.5911 - val_acc: 0.6765\n",
      "Epoch 2/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5950 - acc: 0.6569 - val_loss: 0.5884 - val_acc: 0.6750\n",
      "Epoch 3/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5872 - acc: 0.6606 - val_loss: 0.5876 - val_acc: 0.6750\n",
      "Epoch 4/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5791 - acc: 0.6643 - val_loss: 0.5836 - val_acc: 0.6780\n",
      "Epoch 5/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5720 - acc: 0.6675 - val_loss: 0.5865 - val_acc: 0.6760\n",
      "Epoch 6/10\n",
      "400247/400247 [==============================] - 33s - loss: 0.5655 - acc: 0.6702 - val_loss: 0.5958 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5590 - acc: 0.6732 - val_loss: 0.5875 - val_acc: 0.6804\n",
      "Epoch 8/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5531 - acc: 0.6760 - val_loss: 0.5937 - val_acc: 0.6797\n",
      "Epoch 9/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5484 - acc: 0.6784 - val_loss: 0.5993 - val_acc: 0.6720\n",
      "Epoch 10/10\n",
      "400247/400247 [==============================] - 34s - loss: 0.5428 - acc: 0.6808 - val_loss: 0.5970 - val_acc: 0.6797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x8545483e10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data1, data2], labels, validation_split=VALIDATION_SPLIT,\n",
    "         epochs=10, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2345796, 1)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([test_data_1, test_data_2])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({\"test_id\":test_labels, \"is_duplicate\":preds.ravel()})\n",
    "out_df.to_csv(\"test_predictions_10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
